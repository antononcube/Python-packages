{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0813bc8a730555d",
   "metadata": {},
   "source": [
    "# LLMFunctions\n",
    "\n",
    "## In brief\n",
    "\n",
    "This Python package provides functions and function objects to access, interact, and utilize \n",
    "Large Language Models (LLMs), like OpenAI, [OAI1], and PaLM, [ZG1].\n",
    "\n",
    "The structure and implementation of this Python package close follows the design and implementation\n",
    "of the Raku package \"LLM::Functions\", [AAp1], supported by \"Text::SubParsers\", [AAp4]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a207e0f431302",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## Installation\n",
    "\n",
    "### Install from GitHub\n",
    "\n",
    "```shell\n",
    "pip install -e git+https://github.com/antononcube/Python-packages.git#egg=LLMFunctions-antononcube\\&subdirectory=LLMFunctions\n",
    "```\n",
    "\n",
    "### From PyPi\n",
    "\n",
    "```shell\n",
    "pip install LLMFunctions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209fae5226b13f0",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## Design\n",
    "\n",
    "\"Out of the box\"\n",
    "[\"LLMFunctions\"](https://pypi.org/project/LLMFunctions) uses\n",
    "[\"openai\"](https://pypi.org/project/openai/), [OAIp1], and\n",
    "[\"google-generativeai\"](https://pypi.org/project/google-generativeai/), [GAIp1].\n",
    "Other LLM access packages can be utilized via appropriate LLM configurations.\n",
    "\n",
    "Configurations:\n",
    "- Are instances of the class `LLMFunctions.Configuration`\n",
    "- Are used by instances of the class `LLMFunctions.Evaluator`\n",
    "- Can be converted to dictionary objects (i.e. have a `to_dict` method)\n",
    "\n",
    "New LLM functions are constructed with the function `llm_function`.\n",
    "\n",
    "The function `llm_function`:\n",
    "\n",
    "- Has the option \"llm_evaluator\" that takes evaluators, configurations, or string shorthands as values\n",
    "- Returns anonymous functions (that access LLMs via evaluators/configurations.)\n",
    "- Gives result functions that can be applied to different types of arguments depending on the first argument\n",
    "- Can take a (sub-)parser argument for post-processing of LLM results\n",
    "- Takes as a first argument a prompt that can be a:\n",
    "    - String\n",
    "    - Function with positional arguments\n",
    "    - Function with named arguments\n",
    "\n",
    "Here is a sequence diagram that follows the steps of a typical creation procedure of \n",
    "LLM configuration- and evaluator objects, and the corresponding LLM-function that utilizes them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e522d2b395ca5",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "sequenceDiagram\n",
    "  participant User\n",
    "  participant llmfunc as llm-function\n",
    "  participant llmconf as llm-configuration\n",
    "  participant LLMConf as LLM configuration\n",
    "  participant LLMEval as LLM evaluator\n",
    "  participant AnonFunc as Anonymous function\n",
    "  User ->> llmfunc: ・prompt<br>・conf spec\n",
    "  llmfunc ->> llmconf: conf spec\n",
    "  llmconf ->> LLMConf: conf spec\n",
    "  LLMConf ->> LLMEval: wrap with\n",
    "  LLMEval ->> llmfunc: evaluator object\n",
    "  llmfunc ->> AnonFunc:  create with:<br>・evaluator object<br>・prompt\n",
    "  AnonFunc ->> llmfunc: handle\n",
    "  llmfunc ->> User: handle\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3c894f6ce597dc",
   "metadata": {},
   "source": [
    "Here is a sequence diagram for making a LLM configuration with a global (engineered) prompt,\n",
    "and using that configuration to generate a chat message response:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446fc5986addc920",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "sequenceDiagram\n",
    "  participant WWWOpenAI as WWW::OpenAI\n",
    "  participant User\n",
    "  participant llmfunc as llm-function\n",
    "  participant llmconf as llm-configuration\n",
    "  participant LLMConf as LLM configuration\n",
    "  participant LLMChatEval as LLM chat evaluator\n",
    "  participant AnonFunc as Anonymous function\n",
    "  User ->> llmconf: engineered prompt\n",
    "  llmconf ->> User: configuration object\n",
    "  User ->> llmfunc: ・prompt<br>・configuration object\n",
    "  llmfunc ->> LLMChatEval: configuration object\n",
    "  LLMChatEval ->> llmfunc: evaluator object\n",
    "  llmfunc ->> AnonFunc: create with:<br>・evaluator object<br>・prompt\n",
    "  AnonFunc ->> llmfunc: handle\n",
    "  llmfunc ->> User: handle\n",
    "  User ->> AnonFunc: invoke with<br>message argument\n",
    "  AnonFunc ->> WWWOpenAI: ・engineered prompt<br>・message\n",
    "  WWWOpenAI ->> User: LLM response \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964383c072addf5d",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Configurations\n",
    "\n",
    "### OpenAI-based\n",
    "\n",
    "Here is the default, OpenAI-based configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589d53d7cf603974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:17.339420Z",
     "start_time": "2023-09-26T14:52:16.839432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : 'openai'\n",
      "api_key : None\n",
      "api_user_id : 'user'\n",
      "module : 'openai'\n",
      "model : 'gpt-3.5-turbo-instruct'\n",
      "function : <bound method Completion.create of <class 'openai.api_resources.completion.Completion'>>\n",
      "temperature : 0.2\n",
      "total_probability_cutoff : 0.03\n",
      "max_tokens : 300\n",
      "fmt : 'values'\n",
      "prompts : []\n",
      "prompt_delimiter : ' '\n",
      "stop_tokens : None\n",
      "tools : []\n",
      "tool_prompt : ''\n",
      "tool_request_parser : None\n",
      "tool_response_insertion_function : None\n",
      "argument_renames : {}\n",
      "evaluator : None\n",
      "known_params : ['api_key', 'model', 'prompt', 'suffix', 'max_tokens', 'temperature', 'top_p', 'n', 'stream', 'logprobs', 'stop', 'presence_penalty', 'frequency_penalty', 'best_of', 'logit_bias', 'user']\n",
      "response_object_attribute : None\n",
      "response_value_keys : ['choices', 0, 'text']\n",
      "llm_evaluator : <class 'LLMFunctionObjects.Evaluator.Evaluator'>\n"
     ]
    }
   ],
   "source": [
    "from LLMFunctionObjects import *\n",
    "\n",
    "for k, v in llm_configuration('OpenAI').to_dict().items():\n",
    "    print(f\"{k} : {repr(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972c4905b816406",
   "metadata": {},
   "source": [
    "Here is the ChatGPT-based configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641ecf503e0bc4a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:17.341111Z",
     "start_time": "2023-09-26T14:52:17.337980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : 'chatgpt'\n",
      "api_key : None\n",
      "api_user_id : 'user'\n",
      "module : 'openai'\n",
      "model : 'gpt-3.5-turbo-0613'\n",
      "function : <bound method ChatCompletion.create of <class 'openai.api_resources.chat_completion.ChatCompletion'>>\n",
      "temperature : 0.2\n",
      "total_probability_cutoff : 0.03\n",
      "max_tokens : 300\n",
      "fmt : 'values'\n",
      "prompts : []\n",
      "prompt_delimiter : ' '\n",
      "stop_tokens : None\n",
      "tools : []\n",
      "tool_prompt : ''\n",
      "tool_request_parser : None\n",
      "tool_response_insertion_function : None\n",
      "argument_renames : {}\n",
      "evaluator : None\n",
      "known_params : ['api_key', 'model', 'messages', 'functions', 'function_call', 'temperature', 'top_p', 'n', 'stream', 'logprobs', 'stop', 'presence_penalty', 'frequency_penalty', 'logit_bias', 'user']\n",
      "response_object_attribute : None\n",
      "response_value_keys : ['choices', 0, 'message', 'content']\n",
      "llm_evaluator : <class 'LLMFunctionObjects.EvaluatorChatGPT.EvaluatorChatGPT'>\n"
     ]
    }
   ],
   "source": [
    "for k, v in llm_configuration('ChatGPT').to_dict().items():\n",
    "    print(f\"{k} : {repr(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29da36580b0f16e",
   "metadata": {},
   "source": [
    "**Remark:** `llm_configuration(None)` is equivalent to `llm_configuration('OpenAI')`.\n",
    "\n",
    "**Remark:** Both the \"OpenAI\" and \"ChatGPT\" configuration use functions of the package \"openai\", [OAIp1].\n",
    "The \"OpenAI\" configuration is for text-completions;\n",
    "the \"ChatGPT\" configuration is for chat-completions. \n",
    "\n",
    "### PaLM-based\n",
    "\n",
    "Here is the default PaLM configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8dee29cfffcad36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:17.342816Z",
     "start_time": "2023-09-26T14:52:17.341020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : 'palm'\n",
      "api_key : None\n",
      "api_user_id : 'user'\n",
      "module : 'google.generativeai'\n",
      "model : 'models/text-bison-001'\n",
      "function : <function generate_text at 0x107e33640>\n",
      "temperature : 0.2\n",
      "total_probability_cutoff : 0.03\n",
      "max_tokens : 300\n",
      "fmt : 'values'\n",
      "prompts : []\n",
      "prompt_delimiter : ' '\n",
      "stop_tokens : None\n",
      "tools : []\n",
      "tool_prompt : ''\n",
      "tool_request_parser : None\n",
      "tool_response_insertion_function : None\n",
      "argument_renames : {}\n",
      "evaluator : None\n",
      "known_params : ['model', 'prompt', 'temperature', 'candidate_count', 'max_output_tokens', 'top_p', 'top_k', 'safety_settings', 'stop_sequences', 'client']\n",
      "response_object_attribute : 'result'\n",
      "response_value_keys : []\n",
      "llm_evaluator : <class 'LLMFunctionObjects.Evaluator.Evaluator'>\n"
     ]
    }
   ],
   "source": [
    "for k, v in llm_configuration('PaLM').to_dict().items():\n",
    "    print(f\"{k} : {repr(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e10d4c0e499083",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## Basic usage of LLM functions\n",
    "\n",
    "### Textual prompts\n",
    "\n",
    "Here we make a LLM function with a simple (short, textual) prompt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5db6233bd45d16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:17.347711Z",
     "start_time": "2023-09-26T14:52:17.343852Z"
    }
   },
   "outputs": [],
   "source": [
    "func = llm_function('Show a recipe for:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2ae4497dec84c",
   "metadata": {},
   "source": [
    "Here we evaluate over a message: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c956d2f1cbdb66d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:20.211134Z",
     "start_time": "2023-09-26T14:52:17.346827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Greek Salad Recipe\n",
      "\n",
      "Ingredients:\n",
      "- 1 large head of romaine lettuce, chopped\n",
      "- 1 large cucumber, diced\n",
      "- 1 large tomato, diced\n",
      "- 1/2 red onion, thinly sliced\n",
      "- 1/2 cup kalamata olives, pitted and halved\n",
      "- 1/2 cup crumbled feta cheese\n",
      "- 1/4 cup extra virgin olive oil\n",
      "- 2 tablespoons red wine vinegar\n",
      "- 1 teaspoon dried oregano\n",
      "- Salt and pepper to taste\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a large bowl, combine the chopped romaine lettuce, diced cucumber, diced tomato, sliced red onion, and halved kalamata olives.\n",
      "\n",
      "2. In a small bowl, whisk together the extra virgin olive oil, red wine vinegar, dried oregano, and salt and pepper to make the dressing.\n",
      "\n",
      "3. Pour the dressing over the salad and toss to combine.\n",
      "\n",
      "4. Sprinkle the crumbled feta cheese over the top of the salad.\n",
      "\n",
      "5. Serve immediately or refrigerate until ready to serve.\n",
      "\n",
      "Enjoy your delicious and refreshing Greek salad! You can also add other ingredients such as bell peppers, pepperoncini peppers, or grilled chicken to make it your own. Serve with warm pita bread for a complete meal. \n"
     ]
    }
   ],
   "source": [
    "print(func('greek salad'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d933e26d5ee816",
   "metadata": {},
   "source": [
    "### Positional arguments\n",
    "\n",
    "Here we make a LLM function with a function-prompt and numeric interpreter of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835bbada6a52689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:20.211369Z",
     "start_time": "2023-09-26T14:52:20.208630Z"
    }
   },
   "outputs": [],
   "source": [
    "func2 = llm_function(\n",
    "    lambda a, b: f\"How many {a} can fit inside one {b}?\",\n",
    "    form=float,\n",
    "    llm_evaluator='palm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1107bb766b6ab7c",
   "metadata": {},
   "source": [
    "Here were we apply the function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d7901f722c1ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:20.911543Z",
     "start_time": "2023-09-26T14:52:20.210793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "300.0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2 = func2(\"tennis balls\", \"toyota corolla 2010\")\n",
    "res2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081c7d47898dc3b",
   "metadata": {},
   "source": [
    "\n",
    "Here we show that we got a number:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87356fd747de7470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:20.918956Z",
     "start_time": "2023-09-26T14:52:20.912036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "float"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971573a0edc3f28",
   "metadata": {},
   "source": [
    "### Named arguments\n",
    "\n",
    "Here the first argument is a template with two named arguments: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68aa5babf8a30b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:20.921042Z",
     "start_time": "2023-09-26T14:52:20.917381Z"
    }
   },
   "outputs": [],
   "source": [
    "func3 = llm_function(lambda dish, cuisine: f\"Give a recipe for {dish} in the {cuisine} cuisine.\", llm_evaluator='palm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698dc6035c438075",
   "metadata": {},
   "source": [
    "\n",
    "Here is an invocation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebeebe7528528f8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:23.288833Z",
     "start_time": "2023-09-26T14:52:20.923634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Ingredients:**\n",
      "\n",
      "* 1 head of cabbage (chopped)\n",
      "* 2 carrots (grated)\n",
      "* 1/2 cup of peas (canned or frozen)\n",
      "* 1/2 cup of corn (canned or frozen)\n",
      "* 1/2 cup of mayonnaise\n",
      "* 1/4 cup of sour cream\n",
      "* 1 teaspoon of salt\n",
      "* 1/2 teaspoon of black pepper\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. In a large bowl, combine the cabbage, carrots, peas, and corn.\n",
      "2. In a small bowl, whisk together the mayonnaise, sour cream, salt, and pepper.\n",
      "3. Pour the dressing over the salad and toss to coat.\n",
      "4. Serve immediately or chill for later.\n",
      "\n",
      "**Tips:**\n",
      "\n",
      "* For a more flavorful salad, add some chopped fresh herbs, such as dill or parsley.\n",
      "* You can also add some chopped hard-boiled eggs or shredded chicken to the salad.\n",
      "* If you don't have any fresh vegetables on hand, you can use frozen vegetables instead. Just be sure to thaw them before adding them to the salad.\n",
      "* This salad is best served cold, so make sure to chill it for at least 30 minutes before serving.\n"
     ]
    }
   ],
   "source": [
    "print(func3(dish='salad', cuisine='Russian', max_tokens=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d2edb36dd488de",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## LLM example functions\n",
    "\n",
    "The function `llm_example_function` can be given a training set of examples in order \n",
    "to generating results according to the \"laws\" implied by that training set.  \n",
    "\n",
    "Here a LLM is asked to produce a generalization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f36ab3c196c181fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:23.468219Z",
     "start_time": "2023-09-26T14:52:23.286601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "' leg'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_example_function({'finger': 'hand', 'hand': 'arm'})('foot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b93116597614c",
   "metadata": {},
   "source": [
    "Here is an array of training pairs is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90639baf9b1dded1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:23.672609Z",
     "start_time": "2023-09-26T14:52:23.465690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "' April 22, 1904'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " llm_example_function({\"Einstein\": \"14 March 1879\", \"Pauli\": \"April 25, 1900\"})('Oppenheimer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea968b7a24dc7d",
   "metadata": {},
   "source": [
    "Here is defined a LLM function for translating WL associations into Python dictionaries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7603bace08893423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:26.371387Z",
     "start_time": "2023-09-26T14:52:23.670141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " { 23:3, G:33, T:R5 }\n"
     ]
    }
   ],
   "source": [
    "fea = llm_example_function(('<| A->3, 4->K1 |>', '{ A:3, 4:K1 }'))\n",
    "print(fea('<| 23->3, G->33, T -> R5|>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d5f2c5675af08",
   "metadata": {},
   "source": [
    "The function `llm_example_function` takes as a first argument:\n",
    "- Single `tuple` object of two scalars\n",
    "- A `dict`\n",
    "- A `list` object of pairs (`tuple` objects)\n",
    "\n",
    "**Remark:** The function `llm_example_function` is implemented with `llm-function` and suitable prompt.\n",
    "\n",
    "Here is an example of using hints:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3972d907475bb8b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:26.531014Z",
     "start_time": "2023-09-26T14:52:26.368429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " brown\n"
     ]
    }
   ],
   "source": [
    "fec = llm_example_function(\n",
    "    {\"crocodile\" : \"grasshopper\", \"fox\" : \"cardinal\"},\n",
    "    hint = 'animal colors')\n",
    "\n",
    "print(fec('raccoon'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Synthesizing responses"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cab584d6f4d86f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is an example of prompt synthesis with the function `llm_synthesize` using prompts from the package [\"LLMPrompts\"](https://pypi.org/project/LLMPrompts/):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f3d7da293b0bda9"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Young or old, matters not\n",
      "Age is just a number, hmm\n",
      "Wisdom comes with time\n"
     ]
    }
   ],
   "source": [
    "from LLMPrompts import *\n",
    "\n",
    "print(\n",
    "    llm_synthesize([\n",
    "        llm_prompt(\"Yoda\"), \n",
    "        \"Hi! How old are you?\",\n",
    "        llm_prompt(\"HaikuStyled\")\n",
    "    ]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T14:52:27.407682Z",
     "start_time": "2023-09-26T14:52:26.527582Z"
    }
   },
   "id": "b3ea6733da92572f"
  },
  {
   "cell_type": "markdown",
   "id": "de86ef0de1647ecc",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## Using chat-global prompts\n",
    "\n",
    "The configuration objects can be given prompts that influence the LLM responses \n",
    "\"globally\" throughout the whole chat. (See the second sequence diagram above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b699cba206155",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## Chat objects\n",
    "\n",
    "Here we create chat object that uses OpenAI's ChatGPT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d64c9a7380672f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:53:04.319039Z",
     "start_time": "2023-09-26T14:53:04.304181Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"You are a gem expert and you give concise answers.\"\n",
    "chat = llm_chat(prompt = prompt, chat_id = 'gem-expert-talk', conf = 'ChatGPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3404e3ade7fa079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:53:06.502282Z",
     "start_time": "2023-09-26T14:53:05.930199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'The most transparent gem is diamond.'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.eval('What is the most transparent gem?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ee3b06e7ede2776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:53:10.098101Z",
     "start_time": "2023-09-26T14:53:08.742899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'The second most transparent gem is sapphire, and the third most transparent gem is emerald.'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.eval('Ok. What are the second and third most transparent gems?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc69bfb91ac91a",
   "metadata": {},
   "source": [
    "Here are the prompt(s) and all messages of the chat object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "141c702e9bd1da52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T14:53:14.054874Z",
     "start_time": "2023-09-26T14:53:14.038729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ID: gem-expert-talk\n",
      "------------------------------------------------------------\n",
      "Prompt:\n",
      "You are a gem expert and you give concise answers.\n",
      "------------------------------------------------------------\n",
      "{'role': 'user', 'content': 'What is the most transparent gem?', 'timestamp': 1695739985.929045}\n",
      "------------------------------------------------------------\n",
      "{'role': 'assistant', 'content': 'The most transparent gem is diamond.', 'timestamp': 1695739986.4950988}\n",
      "------------------------------------------------------------\n",
      "{'role': 'user', 'content': 'Ok. What are the second and third most transparent gems?', 'timestamp': 1695739988.741608}\n",
      "------------------------------------------------------------\n",
      "{'role': 'assistant', 'content': 'The second most transparent gem is sapphire, and the third most transparent gem is emerald.', 'timestamp': 1695739990.0806139}\n"
     ]
    }
   ],
   "source": [
    "chat.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbedb61303b36f52",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## References\n",
    "\n",
    "### Articles\n",
    "\n",
    "[AA1] Anton Antonov,\n",
    "[\"Generating documents via templates and LLMs\"](https://rakuforprediction.wordpress.com/2023/07/11/generating-documents-via-templates-and-llms/),\n",
    "(2023),\n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com).\n",
    "\n",
    "[ZG1] Zoubin Ghahramani,\n",
    "[\"Introducing PaLM 2\"](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/),\n",
    "(2023),\n",
    "[Google Official Blog on AI](https://blog.google/technology/ai/).\n",
    "\n",
    "### Repositories, sites\n",
    "\n",
    "[OAI1] OpenAI Platform, [OpenAI platform](https://platform.openai.com/).\n",
    "\n",
    "[WRIr1] Wolfram Research, Inc.\n",
    "[Wolfram Prompt Repository](https://resources.wolframcloud.com/PromptRepository/).\n",
    "\n",
    "### Packages, paclets\n",
    "\n",
    "[AAp1] Anton Antonov,\n",
    "[LLM::Functions Raku package](https://github.com/antononcube/Raku-LLM-Functions),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp2] Anton Antonov,\n",
    "[WWW::OpenAI Raku package](https://github.com/antononcube/Raku-WWW-OpenAI),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp3] Anton Antonov,\n",
    "[WWW::PaLM Raku package](https://github.com/antononcube/Raku-WWW-PaLM),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp4] Anton Antonov,\n",
    "[Text::SubParsers Raku package](https://github.com/antononcube/Raku-Text-SubParsers),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp5] Anton Antonov,\n",
    "[Text::CodeProcessing Raku package](https://github.com/antononcube/Raku-Text-CodeProcessing),\n",
    "(2021),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp6] Anton Antonov,\n",
    "[ML::FindTextualAnswer Raku package](https://github.com/antononcube/Raku-ML-FindTextualAnswer),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp7] Anton Antonov,\n",
    "[ML::NLPTemplateEngine Raku package](https://github.com/antononcube/Raku-ML-NLPTemplateEngine),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp8] Anton Antonov,\n",
    "[LLMPrompts Python package](https://pypi.org/project/LLMPrompts/),\n",
    "(2023),\n",
    "[PyPI.org/antononcube](https://pypi.org/user/antononcube/).\n",
    "\n",
    "[GAIp1] Google AI,\n",
    "[google-generativeai (Google Generative AI Python Client)](https://pypi.org/project/google-generativeai/),\n",
    "(2023),\n",
    "[PyPI.org/google-ai](https://pypi.org/user/google-ai/).\n",
    "\n",
    "[OAIp1] OpenAI, \n",
    "[openai (OpenAI Python Library)](https://pypi.org/project/openai/),\n",
    "(2020-2023),\n",
    "[PyPI.org](https://pypi.org/).\n",
    "\n",
    "[WRIp1] Wolfram Research, Inc.\n",
    "[LLMFunctions paclet](https://resources.wolframcloud.com/PacletRepository/resources/Wolfram/LLMFunctions/),\n",
    "(2023),\n",
    "[Wolfram Language Paclet Repository](https://resources.wolframcloud.com/PacletRepository/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
