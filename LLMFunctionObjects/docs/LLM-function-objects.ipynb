{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLMFunctionObjects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![PyPI](https://img.shields.io/pypi/v/LLMFunctionObjects?label=pypi%20LLMFunctionObjects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![PyPI - Downloads](https://img.shields.io/pypi/dm/LLMFunctionObjects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In brief"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Python package provides functions and function objects to access, interact, and utilize \n",
        "Large Language Models (LLMs), like OpenAI, [OAI1], and Gemini, [ZG1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The structure and implementation of this Python package closely follows the design and implementation\n",
        "of the Raku package \"LLM::Functions\", [AAp1], supported by \"Text::SubParsers\", [AAp4]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*(Here is a [link to the corresponding notebook](https://github.com/antononcube/Python-packages/blob/main/LLMFunctionObjects/docs/LLM-function-objects.ipynb).)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install from GitHub\n",
        "\n",
        "```\n",
        "pip install -e git+https://github.com/antononcube/Python-packages.git#egg=LLMFunctionObjects-antononcube\\&subdirectory=LLMFunctionObjects\n",
        "```\n",
        "\n",
        "### From PyPi\n",
        "\n",
        "```\n",
        "pip install LLMFunctionObjects\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"Out of the box\" [\"LLMFunctionObjects\"](https://pypi.org/project/LLMFunctionObjects) uses [\"openai\"](https://pypi.org/project/openai/), [OAIp1], and [\"google-generativeai\"](https://pypi.org/project/google-generativeai/), [GAIp1], and [\"ollama\"](https://pypi.org/project/ollama/).\n",
        "\n",
        "Other LLM access packages can be utilized via appropriate LLM configurations.\n",
        "\n",
        "Configurations:\n",
        "- Are instances of the class `LLMFunctionObjects.Configuration`\n",
        "- Are used by instances of the class `LLMFunctionObjects.Evaluator`\n",
        "- Can be converted to dictionary objects (i.e. have a `to_dict` method)\n",
        "\n",
        "New LLM functions are constructed with the function `llm_function`.\n",
        "\n",
        "The function `llm_function`:\n",
        "- Produces objects that are set to be \"callable\" (i.e. function objects or functors)\n",
        "- Has the option \"llm_evaluator\" that takes evaluators, configurations, or string shorthands as values\n",
        "- Returns anonymous functions (that access LLMs via evaluators/configurations.)\n",
        "- Gives result functions that can be applied to different types of arguments depending on the first argument\n",
        "- Can take a (sub-)parser argument for post-processing of LLM results\n",
        "- Takes as a first argument a prompt that can be a:\n",
        "    - String\n",
        "    - Function with positional arguments\n",
        "    - Function with named arguments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is a sequence diagram that follows the steps of a typical creation procedure of LLM configuration- and evaluator objects, and the corresponding LLM-function that utilizes them:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\n",
        "sequenceDiagram\n",
        "  participant User\n",
        "  participant llmfunc as llm_function\n",
        "  participant llmconf as llm_configuration\n",
        "  participant LLMConf as LLM configuration\n",
        "  participant LLMEval as LLM evaluator\n",
        "  participant AnonFunc as Function object<br/>(callable)\n",
        "  User ->> llmfunc: ・prompt<br>・conf spec\n",
        "  llmfunc ->> llmconf: conf spec\n",
        "  llmconf ->> LLMConf: conf spec\n",
        "  LLMConf ->> LLMEval: wrap with\n",
        "  LLMEval ->> llmfunc: evaluator object\n",
        "  llmfunc ->> AnonFunc:  create with:<br>・evaluator object<br>・prompt\n",
        "  AnonFunc ->> llmfunc: handle\n",
        "  llmfunc ->> User: handle\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is a sequence diagram for making a LLM configuration with a global (engineered) prompt, and using that configuration to generate a chat message response:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```mermaid\n",
        "sequenceDiagram\n",
        "  participant WWWOpenAI as WWW::OpenAI\n",
        "  participant User\n",
        "  participant llmfunc as llm_function\n",
        "  participant llmconf as llm_configuration\n",
        "  participant LLMConf as LLM configuration\n",
        "  participant LLMChatEval as LLM chat evaluator\n",
        "  participant AnonFunc as Function object<br/>(callable)\n",
        "  User ->> llmconf: engineered prompt\n",
        "  llmconf ->> User: configuration object\n",
        "  User ->> llmfunc: ・prompt<br>・configuration object\n",
        "  llmfunc ->> LLMChatEval: configuration object\n",
        "  LLMChatEval ->> llmfunc: evaluator object\n",
        "  llmfunc ->> AnonFunc: create with:<br>・evaluator object<br>・prompt\n",
        "  AnonFunc ->> llmfunc: handle\n",
        "  llmfunc ->> User: handle\n",
        "  User ->> AnonFunc: invoke with<br>message argument\n",
        "  AnonFunc ->> WWWOpenAI: ・engineered prompt<br>・message\n",
        "  WWWOpenAI ->> User: LLM response \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI-based"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the default, OpenAI-based configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from LLMFunctionObjects import *\n",
        "\n",
        "for k, v in llm_configuration('OpenAI').to_dict().items():\n",
        "    print(f\"{k} : {repr(v)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the ChatGPT-based configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k, v in llm_configuration('ChatGPT').to_dict().items():\n",
        "    print(f\"{k} : {repr(v)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Remark:** `llm_configuration(None)` is equivalent to `llm_configuration('OpenAI')`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Remark:** Both the \"OpenAI\" and \"ChatGPT\" configuration use functions of the package \"openai\", [OAIp1].\n",
        "The \"OpenAI\" configuration is for text-completions;\n",
        "the \"ChatGPT\" configuration is for chat-completions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gemini-based"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the default Gemini configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k, v in llm_configuration('Gemini').to_dict().items():\n",
        "    print(f\"{k} : {repr(v)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ollama-based"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the default Ollama configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k, v in llm_configuration('Ollama').to_dict().items():\n",
        "    print(f\"{k} : {repr(v)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic usage of LLM functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Textual prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we make a LLM function with a simple (short, textual) prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "func = llm_function('Show a recipe for:')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we evaluate over a message: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a simple and classic recipe for Greek Salad:\n",
            "\n",
            "Ingredients:\n",
            "- 3 large tomatoes, cut into wedges\n",
            "- 1 cucumber, sliced\n",
            "- 1 green bell pepper, sliced\n",
            "- 1 red onion, thinly sliced\n",
            "- 200g (about 7 oz) feta cheese, cut into cubes or crumbled\n",
            "- A handful of Kalamata olives\n",
            "- 2 tablespoons extra virgin olive oil\n",
            "- 1 tablespoon red wine vinegar\n",
            "- 1 teaspoon dried oregano\n",
            "- Salt and freshly ground black pepper to taste\n",
            "\n",
            "Instructions:\n",
            "1. In a large bowl, combine the tomatoes, cucumber, green bell pepper, and red onion.\n",
            "2. Add the Kalamata olives and feta cheese on top.\n",
            "3. In a small bowl, whisk together the olive oil, red wine vinegar, oregano, salt, and pepper.\n",
            "4. Pour the dressing over the salad and toss gently to combine.\n",
            "5. Serve immediately, or chill in the refrigerator for 30 minutes to allow the flavors to meld.\n",
            "\n",
            "Enjoy your fresh and flavorful Greek Salad!\n"
          ]
        }
      ],
      "source": [
        "print(func('greek salad'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Local Ollama example (requires a running Ollama server and a pulled model):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ],
      "source": [
        "func_local = llm_function('Answer briefly:', e='Ollama')\n",
        "print(func_local('What is the capital of France?'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we make a LLM function with a function-prompt and numeric interpreter of the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'NumericOnly': {'Name': 'NumericOnly',\n",
              "  'Description': 'Modify results to give numerical responses only',\n",
              "  'PromptText': 'Respond with numerical responses only. Exclude any non-numerical responses. Do not include include any letters. Change words into numbers if possible, for example instead of \"1 billion\" write \"1000000000\".',\n",
              "  'PositionalArguments': [],\n",
              "  'NamedArguments': [],\n",
              "  'Arity': 0,\n",
              "  'Categories': ['Modifier Prompts'],\n",
              "  'Topics': ['Output Formatting'],\n",
              "  'Keywords': ['Numbers', 'Rewrite'],\n",
              "  'ContributedBy': 'Wolfram Staff',\n",
              "  'URL': 'https://resources.wolframcloud.com/PromptRepository/resources/NumericOnly'}}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_prompt_data('NumericOnly')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "func2 = llm_function(\n",
        "    lambda a, b: f\"How many {a} can fit inside one {b}?\" + \n",
        "                \"Respond with numerical responses only. Do not include include any letters. \",\n",
        "    form=float,\n",
        "    llm_evaluator='chatgpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here were we apply the function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41666.0"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res2 = func2(\"tennis balls\", \"toyota corolla 2010\")\n",
        "res2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we show that we got a number:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "float"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(res2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Named arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here the first argument is a template with two named arguments: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "func3 = llm_function(lambda dish, cuisine: f\"Give a recipe for {dish} in the {cuisine} cuisine.\", llm_evaluator='ollama')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is an invocation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A classic Russian salad! Here's a simple and delicious recipe for \"Salad Olga\" or \"Russian Salad\", also known as \"Olivier salad\":\n",
            "\n",
            "**Ingredients:**\n",
            "\n",
            "* 1 cup cooked beef (boiled, diced)\n",
            "* 1 cup cooked chicken (boiled, diced)\n",
            "* 1/2 cup diced hard-boiled egg\n",
            "* 1/2 cup diced onion\n",
            "* 1/4 cup pickled beets (canned or homemade)\n",
            "* 1/4 cup chopped fresh parsley\n",
            "* 1/4 cup chopped fresh dill\n",
            "* 2 tablespoons Russian mustard (or plain Dijon mustard)\n",
            "* Salt and pepper to taste\n",
            "* 2 tablespoons vegetable oil\n",
            "\n",
            "**Instructions:**\n",
            "\n",
            "1. In a large bowl, combine the cooked beef, chicken, egg, onion, pickled beets, parsley, and dill.\n",
            "2. In a small bowl, whisk together the Russian mustard and vegetable oil.\n",
            "3. Pour the dressing over the salad mixture and stir until everything is well combined.\n",
            "4. Season with salt and pepper to taste.\n",
            "5. Cover the bowl with plastic wrap and refrigerate for at least 30 minutes to allow the flavors to meld.\n",
            "\n",
            "**Traditional Variations:**\n",
            "\n",
            "* Some Russian recipes add diced carrots, potatoes, or peas to the salad.\n",
            "* Others use chopped boiled sausage or capers instead of pickled beets.\n",
            "* You can also add a squeeze of fresh lemon juice for extra brightness.\n",
            "\n",
            "**Russian Mustard (Moloko-Syre):**\n",
            "If you can't find Russian mustard, you can make your own by mixing equal parts of milk and vinegar. This creates a creamy and tangy dressing that's a hallmark of traditional Russian salads.\n",
            "\n",
            "Enjoy your delicious and authentic Russian salad!\n"
          ]
        }
      ],
      "source": [
        "print(func3(dish='salad', cuisine='Russian', max_tokens=300))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM example functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function `llm_example_function` can be given a training set of examples in order \n",
        "to generating results according to the \"laws\" implied by that training set.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here a LLM is asked to produce a generalization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The pattern seems to be related to parts of the body where \"hand\" corresponds to \"arm.\" Following this pattern, the output for \"foot\" would be \"leg.\"'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_example_function({'finger': 'hand', 'hand': 'arm'})('foot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is an array of training pairs is used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The output for the input \"Oppenheimer\" would be \"July 22, 1904\".'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_example_function({\"Einstein\": \"14 March 1879\", \"Pauli\": \"April 25, 1900\"})('Oppenheimer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is defined a LLM function for translating WL associations into Python dictionaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: { 23: 3, G: 33, T: R5 }\n"
          ]
        }
      ],
      "source": [
        "fea = llm_example_function(('<| A->3, 4->K1 |>', '{ A:3, 4:K1 }'))\n",
        "print(fea('<| 23->3, G->33, T -> R5|>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function `llm_example_function` takes as a first argument:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Single `tuple` object of two scalars\n",
        "- A `dict`\n",
        "- A `list` object of pairs (`tuple` objects)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Remark:** The function `llm_example_function` is implemented with `llm_function` and suitable prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is an example of using hints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raccoon - black and white\n"
          ]
        }
      ],
      "source": [
        "fec = llm_example_function(\n",
        "    {\"crocodile\" : \"grasshopper\", \"fox\" : \"cardinal\"},\n",
        "    hint = 'animal colors', e = 'ollama')\n",
        "\n",
        "print(fec('raccoon'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Synthesizing responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is an example of prompt synthesis with the function `llm_synthesize` using prompts from the package [\"LLMPrompts\"](https://pypi.org/project/LLMPrompts/), [AAp8]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Old, I am, yes.\n",
            "Wisdom deep as stars above,\n",
            "Time flows, I endure.\n"
          ]
        }
      ],
      "source": [
        "from LLMPrompts import *\n",
        "\n",
        "print(\n",
        "    llm_synthesize([\n",
        "        llm_prompt(\"Yoda\"), \n",
        "        \"Hi! How old are you?\",\n",
        "        llm_prompt(\"HaikuStyled\")\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using chat-global prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The configuration objects can be given prompts that influence the LLM responses \n",
        "\"globally\" throughout the whole chat. (See the second sequence diagram above.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we create chat object that uses OpenAI's ChatGPT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"You are a gem expert and you give concise answers.\"\n",
        "chat = llm_chat(prompt = prompt, chat_id = 'gem-expert-talk', conf = 'ChatGPT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most transparent gem is typically considered to be diamond. Diamonds have exceptional clarity and brilliance, making them highly transparent and prized in jewelry.'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat.eval('What is the most transparent gem?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The second most transparent gem is usually white sapphire, known for its clarity and durability. The third most transparent gem is often zircon, which has high brilliance and good transparency.'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat.eval('Ok. What are the second and third most transparent gems?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are the prompt(s) and all messages of the chat object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat ID: gem-expert-talk\n",
            "------------------------------------------------------------\n",
            "Prompt:\n",
            "You are a gem expert and you give concise answers.\n",
            "------------------------------------------------------------\n",
            "{'role': 'user', 'content': 'What is the most transparent gem?', 'timestamp': 1771017949.900129}\n",
            "------------------------------------------------------------\n",
            "{'role': 'assistant', 'content': 'The most transparent gem is typically considered to be diamond. Diamonds have exceptional clarity and brilliance, making them highly transparent and prized in jewelry.', 'timestamp': 1771017951.720435}\n",
            "------------------------------------------------------------\n",
            "{'role': 'user', 'content': 'Ok. What are the second and third most transparent gems?', 'timestamp': 1771017951.7310028}\n",
            "------------------------------------------------------------\n",
            "{'role': 'assistant', 'content': 'The second most transparent gem is usually white sapphire, known for its clarity and durability. The third most transparent gem is often zircon, which has high brilliance and good transparency.', 'timestamp': 1771017953.605421}\n"
          ]
        }
      ],
      "source": [
        "chat.print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Potential and known problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The LLM frameworks of OpenAI and Google are changed often, which produces problems that are generally three types:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Implementation based on obsolete design of the corresponding Python packages for accessing LLMs\n",
        "- Obsolete models\n",
        "- Obsolete signatures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Currently, for Google's Gemini the method \"ChatGemini\" works better than the method \"Gemini\".\n",
        "Note, that PaLM itself is considered legacy by Google and is replaced with Gemini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generally, speaking prefer using the \"Chat\" prefixed methods: \"ChatGPT\" and \"ChatGemini\".\n",
        "(OpenAI does/did say that it \"simple\" text completion models are obsoleted.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[AA1] Anton Antonov,\n",
        "[\"Generating documents via templates and LLMs\"](https://rakuforprediction.wordpress.com/2023/07/11/generating-documents-via-templates-and-llms/),\n",
        "(2023),\n",
        "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[ZG1] Google AI,\n",
        "[\"Gemini models overview\"](https://ai.google.dev/gemini-api/docs/models/gemini),\n",
        "(2024),\n",
        "[Google AI documentation](https://ai.google.dev/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repositories, sites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[OAI1] OpenAI Platform, [OpenAI platform](https://platform.openai.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[WRIr1] Wolfram Research, Inc.\n",
        "[Wolfram Prompt Repository](https://resources.wolframcloud.com/PromptRepository/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Packages, paclets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[AAp1] Anton Antonov,\n",
        "[LLM::Functions Raku package](https://github.com/antononcube/Raku-LLM-Functions),\n",
        "(2023),\n",
        "[GitHub/antononcube](https://github.com/antononcube)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[AAp2] Anton Antonov,\n",
        "[WWW::OpenAI Raku package](https://github.com/antononcube/Raku-WWW-OpenAI),\n",
        "(2023),\n",
        "[GitHub/antononcube](https://github.com/antononcube)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[AAp3] Anton Antonov,\n",
        "[WWW::PaLM Raku package](https://github.com/antononcube/Raku-WWW-PaLM),\n",
        "(2023),\n",
        "[GitHub/antononcube](https://github.com/antononcube)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[AAp4] Anton Antonov,\n",
        "[Text::SubParsers Raku package](https://github.com/antononcube/Raku-Text-SubParsers),\n",
        "(2023),\n",
        "[GitHub/antononcube](https://github.com/antononcube)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[AAp5] Anton Antonov,\n",
        "[Text::CodeProcessing Raku package](https://github.com/antononcube/Raku-Text-CodeProcessing),\n",
        "(2021),\n",
        "[GitHub/antononcube](https://github.com/antononcube)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[AAp6] Anton Antonov,\n",
        "[ML::FindTextualAnswer Raku package](https://github.com/antononcube/Raku-ML-FindTextualAnswer),\n",
        "(2023),\n",
        "[GitHub/antononcube](https://github.com/antononcube)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[AAp7] Anton Antonov,\n",
        "[ML::NLPTemplateEngine Raku package](https://github.com/antononcube/Raku-ML-NLPTemplateEngine),\n",
        "(2023),\n",
        "[GitHub/antononcube](https://github.com/antononcube)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[AAp8] Anton Antonov,\n",
        "[LLMPrompts Python package](https://pypi.org/project/LLMPrompts/),\n",
        "(2023),\n",
        "[PyPI.org/antononcube](https://pypi.org/user/antononcube/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[GAIp1] Google AI,\n",
        "[google-generativeai (Google Generative AI Python Client)](https://pypi.org/project/google-generativeai/),\n",
        "(2023),\n",
        "[PyPI.org/google-ai](https://pypi.org/user/google-ai/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[OAIp1] OpenAI, \n",
        "[openai (OpenAI Python Library)](https://pypi.org/project/openai/),\n",
        "(2020-2023),\n",
        "[PyPI.org](https://pypi.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[WRIp1] Wolfram Research, Inc.\n",
        "[LLMFunctions paclet](https://resources.wolframcloud.com/PacletRepository/resources/Wolfram/LLMFunctionObjects/),\n",
        "(2023),\n",
        "[Wolfram Language Paclet Repository](https://resources.wolframcloud.com/PacletRepository/)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SciPyCentric",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
